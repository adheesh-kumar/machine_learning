{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d84yx0L4UGP"
      },
      "source": [
        "Exercise 2:\n",
        "\n",
        "Python Version: 3.7\n",
        "all newest versions of pytorch except for transforms.version=v1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey2VKRZ54UGR",
        "outputId": "bca6dacf-20a5-4d22-8304-a6d3c6ef542a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=100, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "trainStep = len(train_loader.dataset) // 100\n",
        "testStep = len(test_loader.dataset) // 100\n",
        "\n",
        "\n",
        "# VGG11 model\n",
        "class VGG11(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG11, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 10),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cnn(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ6mfa8p4UGS",
        "outputId": "98b550ec-895a-46a6-d547-16aaa60bb0bd"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] initializing CNN...\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = VGG11().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "stats = {\n",
        "\t\"train_loss\": [],\n",
        "\t\"train_acc\": [],\n",
        "\t\"test_loss\": [],\n",
        "\t\"test_acc\": []\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVwdPK7y_rs0",
        "outputId": "e7763ad0-ec9b-49a7-e2d0-eafbe8b5d82b"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(\"[INFO] training the network...\")\n",
        "startTime = time.time()\n",
        "\n",
        "\n",
        "# Training and Testing loop\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    trainLoss = 0\n",
        "    trainCorrect = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        trainLoss += loss.item()\n",
        "        trainCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    doneTraining = time.time()\n",
        "    print(\"[INFO] epoch time taken to train the model: {:.2f}s\".format(doneTraining - startTime))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    testLoss = 0\n",
        "    testCorrect = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "\n",
        "            testLoss += loss.item()\n",
        "            testCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "\n",
        "    # calculate the average training and validation loss\n",
        "    avgTrainLoss = trainLoss / trainStep\n",
        "    avgTestLoss = testLoss / testStep\n",
        "    # calculate the training and validation accuracy\n",
        "    trainCorrect = trainCorrect / len(train_loader.dataset)\n",
        "    testCorrect = testCorrect / len(test_loader.dataset)\n",
        "    # update our training history\n",
        "    stats[\"train_loss\"].append(avgTrainLoss)\n",
        "    stats[\"train_acc\"].append(trainCorrect)\n",
        "    stats[\"test_loss\"].append(avgTestLoss)\n",
        "    stats[\"test_acc\"].append(testCorrect)\n",
        "\n",
        "    print(f\"Done Train and Test Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, trainCorrect))\n",
        "    print(\"Test loss: {:.6f}, Test accuracy: {:.4f}\\n\".format(avgTestLoss, testCorrect))\n",
        "\n",
        "endTime = time.time()\n",
        "print(\"\\n\\n[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n",
        "\n",
        "print(stats)\n",
        "\n",
        "torch.save(model, \"trained_model.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7UhNCen4EBA"
      },
      "source": [
        "\n",
        "\n",
        "'train_loss': [1.5673993885517121, 1.4748803796370824, 1.471510537068049, 1.4692868530750274, 1.4681892536083858]\n",
        "\n",
        "'train_acc': [0.9015833333333333, 0.9872, 0.9902666666666666, 0.9924833333333334, 0.9932666666666666]\n",
        "\n",
        "'test_loss': [1.4757724356651307, 1.4715722870826722, 1.469182391166687, 1.4686590445041656, 1.4679065442085266]\n",
        "\n",
        "'test_acc': [0.9867, 0.9901, 0.9924, 0.9929, 0.9933]\n",
        "\n",
        "---\n",
        "\n",
        "NOTE: pngs attatched seperately in submission\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5N98p5CtNFt4",
        "outputId": "8e495024-2322-4414-a726-a61dd609e028"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for key in stats:\n",
        "  plt.figure()\n",
        "  plt.plot(stats[key], label=key)\n",
        "  plt.title(f\"{key} on dataset\")\n",
        "  plt.xlabel(\"Epoch #\")\n",
        "  plt.ylabel(\"Loss/Accuracy\")\n",
        "  plt.savefig(key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mRRmFXEwg6s",
        "outputId": "78bf05df-d75a-4461-f895-4ebca919d2ec"
      },
      "outputs": [],
      "source": [
        "#PART B)\n",
        "\n",
        "\n",
        "# Data preprocessing\n",
        "transformHorizontal = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.RandomHorizontalFlip(p=1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "transformVertical = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.RandomVerticalFlip(p=1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# MNIST dataset\n",
        "htest_dataset = datasets.MNIST(root='./data', train=False, transform=transformHorizontal)\n",
        "vtest_dataset = datasets.MNIST(root='./data', train=False, transform=transformVertical)\n",
        "\n",
        "# Data loaders\n",
        "htest_loader = DataLoader(dataset=htest_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "vtest_loader = DataLoader(dataset=vtest_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "htestStep = len(htest_loader.dataset) // 100\n",
        "vtestStep = len(vtest_loader.dataset) // 100\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "htestLoss = 0\n",
        "htestCorrect = 0\n",
        "vtestLoss = 0\n",
        "vtestCorrect = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in htest_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "        htestLoss += loss.item()\n",
        "        htestCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    for x, y in vtest_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "        vtestLoss += loss.item()\n",
        "        vtestCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "\n",
        "# calculate the average training and validation loss\n",
        "avghTestLoss = htestLoss / htestStep\n",
        "avgvTestLoss = vtestLoss / vtestStep\n",
        "# calculate the training and validation accuracy\n",
        "htestCorrect = htestCorrect / len(htest_loader.dataset)\n",
        "vtestCorrect = vtestCorrect / len(vtest_loader.dataset)\n",
        "\n",
        "bstats = {\n",
        "\t\"htest_loss\": [],\n",
        "\t\"htest_acc\": [],\n",
        "\t\"vtest_loss\": [],\n",
        "\t\"vtest_acc\": []\n",
        "}\n",
        "\n",
        "# update our training history\n",
        "bstats[\"htest_loss\"].append(avghTestLoss)\n",
        "bstats[\"htest_acc\"].append(htestCorrect)\n",
        "bstats[\"vtest_loss\"].append(avgvTestLoss)\n",
        "bstats[\"vtest_acc\"].append(vtestCorrect)\n",
        "\n",
        "\n",
        "print(bstats)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVEjxDiA39Qk"
      },
      "source": [
        "\n",
        "\n",
        "'htest_loss': [0.34446121712525685], 'htest_acc': [0.3898]\n",
        "'vtest_loss': [2.047345280647278], 'vtest_acc': [0.4096]\n",
        "\n",
        "some numbers when flipped as we have can be viewed as the same number. For example horizontal flips on {8, 0, 1} could all be read as such when flipped horizontally, some on the vertical flip for numbers like {8, 0, 3}. So since we are getting accuracies of ~0.4 it seems reasonable that only a percentage of classes will remain interpretable after the transform.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwGYDAg164kD",
        "outputId": "36c78758-8fb8-4b3f-fc24-8c9d045f9fb7"
      },
      "outputs": [],
      "source": [
        "#PART Bii)\n",
        "\n",
        "# Data preprocessing\n",
        "transform1 = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x : x + 0.01*torch.randn_like(x)),\n",
        "])\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x : x + 0.1*torch.randn_like(x)),\n",
        "])\n",
        "\n",
        "\n",
        "transform3 = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x : x + 1*torch.randn_like(x)),\n",
        "])\n",
        "\n",
        "# MNIST dataset\n",
        "test1_dataset = datasets.MNIST(root='./data', train=False, transform=transform1)\n",
        "test2_dataset = datasets.MNIST(root='./data', train=False, transform=transform2)\n",
        "test3_dataset = datasets.MNIST(root='./data', train=False, transform=transform3)\n",
        "\n",
        "# Data loaders\n",
        "test1_loader = DataLoader(dataset=test1_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "test2_loader = DataLoader(dataset=test2_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "test3_loader = DataLoader(dataset=test3_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "test1Step = len(test1_loader.dataset) // 100\n",
        "test2Step = len(test2_loader.dataset) // 100\n",
        "test3Step = len(test3_loader.dataset) // 100\n",
        "\n",
        "step = [test1Step, test2Step, test3Step]\n",
        "sizes = [len(test1_loader.dataset), len(test2_loader.dataset), len(test3_loader.dataset)]\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "\n",
        "testLoss = [0, 0, 0]\n",
        "testCorrect = [0, 0, 0]\n",
        "\n",
        "loaders = [test1_loader, test2_loader, test3_loader]\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for i in range(3):\n",
        "\n",
        "        for x, y in loaders[i]:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "\n",
        "            testLoss[i] += loss.item()\n",
        "            testCorrect[i] += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "\n",
        "#avgTestLoss = [testLoss[i]/step[i] for i in range(3)]\n",
        "\n",
        "avgTestLoss = [testLoss[0]/step[0],testLoss[1]/step[1],testLoss[2]/step[2]]\n",
        "avgTestCorrect = [testCorrect[0]/sizes[0],testCorrect[1]/sizes[1],testCorrect[2]/sizes[2]]\n",
        "\n",
        "\n",
        "biistats = {\n",
        "\t\"0.01test_loss\": [],\n",
        "\t\"0.01test_acc\": [],\n",
        "\t\"0.1test_loss\": [],\n",
        "\t\"0.1test_acc\": [],\n",
        "\t\"1test_loss\": [],\n",
        "\t\"1test_acc\": []\n",
        "}\n",
        "\n",
        "# update our training history\n",
        "biistats[\"0.01test_loss\"].append(avgTestLoss[0])\n",
        "biistats[\"0.01test_acc\"].append(avgTestCorrect[0])\n",
        "biistats[\"0.1test_loss\"].append(avgTestLoss[1])\n",
        "biistats[\"0.1test_acc\"].append(avgTestCorrect[1])\n",
        "biistats[\"1test_loss\"].append(avgTestLoss[2])\n",
        "biistats[\"1test_acc\"].append(avgTestCorrect[2])\n",
        "\n",
        "\n",
        "print(biistats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boqNP6stFrPX"
      },
      "source": [
        "**effect:** as we test on exponentially increasing variance - [0.01, 0.1, 1] we see steeper and steeper decline in accuracy [0.99, 0.96, 0.1] so the more gaussian noise the worse the accuracy\n",
        "\n",
        "'0.01test_loss': [1.4728576004505158],\n",
        "'0.01test_acc': [0.9882],\n",
        "\n",
        "'0.1test_loss': [1.504005583524704],\n",
        "'0.1test_acc': [0.9588],\n",
        "\n",
        "'1test_loss': [2.362187247276306],\n",
        "'1test_acc': [0.0979]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZq_xHm-HeA_",
        "outputId": "dc1bd542-03e5-48ec-f4e6-ccd1b97d1372"
      },
      "outputs": [],
      "source": [
        "#PART C\n",
        "\n",
        "# Instantiate the model\n",
        "model2 = VGG11().to(device)\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform1 = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x : x + 0.01*torch.randn_like(x)),\n",
        "])\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x : x + 0.1*torch.randn_like(x)),\n",
        "])\n",
        "\n",
        "\n",
        "transform3 = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x : x + 1*torch.randn_like(x)),\n",
        "])\n",
        "\n",
        "\n",
        "mtransforms = [transform, transform1, transform2, transform3]\n",
        "\n",
        "mdatasets = []\n",
        "loaders = []\n",
        "\n",
        "for i in range(len(mtransforms)):\n",
        "    mdatasets.append(datasets.MNIST(root='./data', train=True, transform=mtransforms[i]))\n",
        "    loaders.append(DataLoader(dataset=mdatasets[i], batch_size=100, shuffle=True, num_workers=2))\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "crit = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model2.train()\n",
        "    trainLoss = 0\n",
        "    trainCorrect = 0\n",
        "\n",
        "    for loader in loaders:\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            pred = model2(x)\n",
        "            loss = crit(pred, y)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1toQYQKQkAv",
        "outputId": "9a8ad2f1-84cf-4d0e-ff83-dcc40d03bcfb"
      },
      "outputs": [],
      "source": [
        "#PART C TESTING)\n",
        "\n",
        "\n",
        "# Data preprocessing\n",
        "transformHorizontal = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.RandomHorizontalFlip(p=1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "transformVertical = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.RandomVerticalFlip(p=1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# MNIST dataset\n",
        "htest_dataset = datasets.MNIST(root='./data', train=False, transform=transformHorizontal)\n",
        "vtest_dataset = datasets.MNIST(root='./data', train=False, transform=transformVertical)\n",
        "\n",
        "# Data loaders\n",
        "htest_loader = DataLoader(dataset=htest_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "vtest_loader = DataLoader(dataset=vtest_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "htestStep = len(htest_loader.dataset) // 100\n",
        "vtestStep = len(vtest_loader.dataset) // 100\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "htestLoss = 0\n",
        "htestCorrect = 0\n",
        "vtestLoss = 0\n",
        "vtestCorrect = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in htest_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "        htestLoss += loss.item()\n",
        "        htestCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    for x, y in vtest_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "        vtestLoss += loss.item()\n",
        "        vtestCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "\n",
        "# calculate the average training and validation loss\n",
        "avghTestLoss = htestLoss / htestStep\n",
        "avgvTestLoss = vtestLoss / vtestStep\n",
        "# calculate the training and validation accuracy\n",
        "htestCorrect = htestCorrect / len(htest_loader.dataset)\n",
        "vtestCorrect = vtestCorrect / len(vtest_loader.dataset)\n",
        "\n",
        "bstats = {\n",
        "\t\"htest_loss\": [],\n",
        "\t\"htest_acc\": [],\n",
        "\t\"vtest_loss\": [],\n",
        "\t\"vtest_acc\": []\n",
        "}\n",
        "\n",
        "# update our training history\n",
        "bstats[\"htest_loss\"].append(avghTestLoss)\n",
        "bstats[\"htest_acc\"].append(htestCorrect)\n",
        "bstats[\"vtest_loss\"].append(avgvTestLoss)\n",
        "bstats[\"vtest_acc\"].append(vtestCorrect)\n",
        "\n",
        "\n",
        "print(bstats)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#ii\n",
        "\n",
        "#PART Cii)\n",
        "\n",
        "# Data preprocessing\n",
        "transform1 = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x : x + 0.01*torch.randn_like(x)),\n",
        "])\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x : x + 0.1*torch.randn_like(x)),\n",
        "])\n",
        "\n",
        "\n",
        "transform3 = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x : x + 1*torch.randn_like(x)),\n",
        "])\n",
        "\n",
        "# MNIST dataset\n",
        "test1_dataset = datasets.MNIST(root='./data', train=False, transform=transform1)\n",
        "test2_dataset = datasets.MNIST(root='./data', train=False, transform=transform2)\n",
        "test3_dataset = datasets.MNIST(root='./data', train=False, transform=transform3)\n",
        "\n",
        "# Data loaders\n",
        "test1_loader = DataLoader(dataset=test1_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "test2_loader = DataLoader(dataset=test2_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "test3_loader = DataLoader(dataset=test3_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "test1Step = len(test1_loader.dataset) // 100\n",
        "test2Step = len(test2_loader.dataset) // 100\n",
        "test3Step = len(test3_loader.dataset) // 100\n",
        "\n",
        "step = [test1Step, test2Step, test3Step]\n",
        "sizes = [len(test1_loader.dataset), len(test2_loader.dataset), len(test3_loader.dataset)]\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "\n",
        "testLoss = [0, 0, 0]\n",
        "testCorrect = [0, 0, 0]\n",
        "\n",
        "loaders = [test1_loader, test2_loader, test3_loader]\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for i in range(3):\n",
        "\n",
        "        for x, y in loaders[i]:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "\n",
        "            testLoss[i] += loss.item()\n",
        "            testCorrect[i] += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "\n",
        "#avgTestLoss = [testLoss[i]/step[i] for i in range(3)]\n",
        "\n",
        "avgTestLoss = [testLoss[0]/step[0],testLoss[1]/step[1],testLoss[2]/step[2]]\n",
        "avgTestCorrect = [testCorrect[0]/sizes[0],testCorrect[1]/sizes[1],testCorrect[2]/sizes[2]]\n",
        "\n",
        "\n",
        "biistats = {\n",
        "\t\"0.01test_loss\": [],\n",
        "\t\"0.01test_acc\": [],\n",
        "\t\"0.1test_loss\": [],\n",
        "\t\"0.1test_acc\": [],\n",
        "\t\"1test_loss\": [],\n",
        "\t\"1test_acc\": []\n",
        "}\n",
        "\n",
        "# update our training history\n",
        "biistats[\"0.01test_loss\"].append(avgTestLoss[0])\n",
        "biistats[\"0.01test_acc\"].append(avgTestCorrect[0])\n",
        "biistats[\"0.1test_loss\"].append(avgTestLoss[1])\n",
        "biistats[\"0.1test_acc\"].append(avgTestCorrect[1])\n",
        "biistats[\"1test_loss\"].append(avgTestLoss[2])\n",
        "biistats[\"1test_acc\"].append(avgTestCorrect[2])\n",
        "\n",
        "\n",
        "print(biistats)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab-HhG8VXSs2"
      },
      "source": [
        "\n",
        "\n",
        "'htest_loss': [2.0139254558086397],\n",
        "'htest_acc': [0.4559],\n",
        "\n",
        "\n",
        "'vtest_loss': [2.1255094814300537]\n",
        "'vtest_acc': [0.3309]\n",
        "\n",
        "'0.01test_loss': [1.5470370030403138]\n",
        "'0.01test_acc': [0.9315]\n",
        "\n",
        "'0.1test_loss': [1.5448921930789947]\n",
        "'0.1test_acc': [0.9329]\n",
        "\n",
        "'1test_loss': [1.6545295405387879]\n",
        "'1test_acc': [0.8307]\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "We used a total of 6 training sets: {regular, horizontalFlip, verticalFlip, GausianNoise(GN)-0.01, GN-0.1, GN-1} in training the model with data augmentation\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
